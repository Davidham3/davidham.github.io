<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=6.0.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=6.0.0">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=6.0.0" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="categories">
<meta property="og:url" content="http://yoursite.com/categories/index.html">
<meta property="og:site_name" content="Davidham&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-02-19T13:29:39.253Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="categories">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Gemini',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/categories/"/>





  <title>Image Style Transfer Using Convolutional Neural Networks 阅读笔记 | Davidham's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Davidham's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Davidham">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Davidham's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Image Style Transfer Using Convolutional Neural Networks 阅读笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-24T23:51:39+08:00">2018-02-24</time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/论文阅读笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>大体原理：选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。<br><a id="more"></a><br>论文：Gatys L A, Ecker A S, Bethge M. Image Style Transfer Using Convolutional Neural Networks[C]// Computer Vision and Pattern Recognition. IEEE, 2016:2414-2423.</p>
<h1 id="Image-Style-Transfer-Using-Convolutional-Neural-Networks"><a href="#Image-Style-Transfer-Using-Convolutional-Neural-Networks" class="headerlink" title="Image Style Transfer Using Convolutional Neural Networks"></a>Image Style Transfer Using Convolutional Neural Networks</h1><h1 id="大体原理"><a href="#大体原理" class="headerlink" title="大体原理"></a>大体原理</h1><p>选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。</p>
<h1 id="Deep-image-representations"><a href="#Deep-image-representations" class="headerlink" title="Deep image representations"></a>Deep image representations</h1><p>We used the feature space provided by a normalized version of the 16 convolutional and 5 pooling layers of the 19-layer VGG network. We normalized the network by scaling the weights such that the mean activation of each convolutional filter over images and positions is equal to one. Such re-scaling can be done for the VGG network without changing its output, because it contains only rectifying linear activation functions and no normalization or pooling over feature maps.<br>其实这里我不是很明白为什么不会影响输出。</p>
<h2 id="content-representation"><a href="#content-representation" class="headerlink" title="content representation"></a>content representation</h2><p>A layer with $N_l$ distinct filters has $N_l$ feature maps each of size $M_l$, where $M_l$ is the height times the width of the feature map. So the responses in a layer $l$ can be stored in a matrix $F^l \in \mathcal{R}^{N_l \times M_l}$ where $F^l_{ij}$ is the activation of the $i^{th}$ filter at position $j$ in layer $l$.<br>Let $\vec{p}$ and $\vec{x}$ be the original image and the image that is generated, and $P^l$ and $F^l$ their respective feature representation in layer $l$.<br>We then define the squared-error loss between the two feature representations</p>
<script type="math/tex; mode=display">\mathcal{L}_{content}(\vec{p}, \vec{x}, l) = \frac{1}{2}\sum_{i, j}(F^l_{ij}-P^l_{ij})^2</script><p>The derivative of this loss with respect to the activations in layer $l$ equals</p>
<p>\begin{equation}<br>\frac{\partial{\mathcal{L}_{content}}}{\partial{F^l_{ij}}}=\left\{<br>\begin{aligned}<br>&amp; (F^l - P^l)_{ij} &amp; if \ F^l_{ij} &gt; 0 \\<br>&amp; 0 &amp; if \ F^l_{ij} &lt; 0<br>\end{aligned}<br>\right.<br>\end{equation}</p>
<p>from which the gradient with respect to the image $\vec{x}$ can be computed using standard error back-propagation.</p>
<p>When Convolutional Neural Networks are trained on object recongnition, they develop a representation of the image that makes object information increasingly explicit along the processing hierarchy. Higher layers in the network capture the high-level <em>content</em> in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruction very much. We therefore refer to the feature responses in higher layers of the network as the <em>content representation</em>.</p>
<h2 id="style-representation"><a href="#style-representation" class="headerlink" title="style representation"></a>style representation</h2><p>To obtain a representation of the style of an input image, we use a feature space designed to capture texture information. This feature space can be built on top of the filter responses in any layer of the network. It consists of the correlations between the different filter responses, where the expecation is taken over the spatial extent of the feature maps. These feature correlations are given by the Gram matrix $G^l \in \mathcal{R}^{N_L \times N_l}$, where $G^l_{ij}$ is the inner product between the vecotrized feature maps $i$ and $j$ in layer $l$:</p>
<script type="math/tex; mode=display">G^l_{ij}=\sum_kF^l_{ik}F^l_{jk}.</script><p>By inducing the feature corelations of multiple layers, we obtain a stationary, multi-scale representation of the input image, which captures its texture information but not the global arrangement. Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image. This is done by using gradient descent from a white noise image to minimise the mean-squared distance between the entries of the Gram matrices from the original image and the Gram matrices of the image to be generated.<br>Let $\vec{a}$ and $\vec{x}$ be the original image and the image that is generated, and $A^l$ and $G^l$ their respective style representation in layer $l$. The contribution of layer $l$ to the toal loss is then</p>
<script type="math/tex; mode=display">E_l = \frac{1}{4N^2_lM^2_l}\sum_{i,j}(G^l_{ij} - A^l_{ij})^2</script><p>and the total style loss is</p>
<script type="math/tex; mode=display">\mathcal{L}_{style}(\vec{a}, \vec{x})=\sum^L_{l=0}w_lE_l,</script><p>where $w_L$ are weighting factors of the contribution of each layer to the total loss (see below for specific values of $w_l$ in our results). The derivative of $E_l$ with respect to the activations in layer $l$ can be computed analytically:</p>
<p>\begin{equation}<br>\frac{\partial{E_l}}{\partial{F^l_{ij}}}=\left\{<br>\begin{aligned}<br>&amp; \frac{1}{N^2_lM^2_l}((F^l)^T(G^l-A^l))_{ji} &amp; if \ F^l_{ij} &gt; 0 \\<br>&amp; 0 &amp; if \ F^l_{ij} &lt; 0<br>\end{aligned}<br>\right.<br>\end{equation}<br>The gradient of $E_l$ with respect to the pixel values $\vec{x}$ can be readily computed using standard error back-propagation.</p>
<h2 id="style-transfer"><a href="#style-transfer" class="headerlink" title="style transfer"></a>style transfer</h2><p>To transfer the style of an artwork $\vec{a}$ onto a photograph $\vec{p}$ we synthesise a new image that simultaneously matches the content representation of $\vec{p}$ and the style representation of $\vec{a}$. Thus we jointly minimise the distance of the feature representations of a white noise image fron the content representation of the photograph in one layer and the style representation of the painting defined on a numebr of layers of the Convolutional Neural Network. The loss function we minimise is</p>
<script type="math/tex; mode=display">\mathcal{L}_{total}(\vec{p}, \vec{a}, \vec{x})=\alpha \mathcal{L}_{content}(\vec{p}, \vec{x}) + \beta \mathcal{L}_{style}(\vec{a}, \vec{x})</script><p>where $\alpha$ and $\beta$ are the weighting factors for content and style reconstruction, respectively. The gradient with respect to the pixel values $\frac{\partial{\mathcal{L}_{total}}}{\partial{\vec{x}}}$ can be used as input for some numerical optimisation strategy. Here we use <strong>L-BFGS</strong>, which we found to work best for image synthesis. To extract image information on comparable scales, we always resized the style image to the same size as the content image before computing its feature representations.</p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="Trade-off-between-content-and-style-matching"><a href="#Trade-off-between-content-and-style-matching" class="headerlink" title="Trade-off between content and style matching"></a>Trade-off between content and style matching</h2><p>Since the loss function we minimise during image synthesis is a linear combination between the loss functions for content and style respectively, we can smoothly regulate the emphasis on either reconstructing the content or the style(Fig4).<br><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/Fig4.PNG" alt="Fig4"></p>
<center>Figure 4. Relative weighting of matching content and style of the respective source images. The ratio $\alpha / \beta$ between matching the content and matching the style increases from top left to bottom right. A high emphasis on the style effectively produces a texturised version of the style image(top left). A high emphasis on the content produces an image with only little stylisation(bottom right). In practice one can smoothly interpolate between the two extremes.</center>

<h1 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h1><p>关于实现的部分，我自己用mxnet实现了一下，但是发现和mxnet的example里面给的非常不一样。在他们的实现里面提到了Total variation denoising.</p>
<h2 id="Total-variation-denoising"><a href="#Total-variation-denoising" class="headerlink" title="Total variation denoising"></a>Total variation denoising</h2><p>In signal processing, total variation denoising, also known as total variation regularization, is a process, most often used in digital image processing, that has applications in noise removal.<br><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/ROF_Denoising_Example.png" alt="Total variation denoising"></p>
<p><center>Example of application of the Rudin et al.[1] total variation denoising technique to an image corrupted by Gaussian noise. This example created using demo_tv.m by Guy Gilboa, see external links.</center><br>It is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signla is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges. The concept was pioneered by Rudin, Osher, and Fatemi in 1992 and so is today known as the ROF model.<br>This noise removal technique has advantages over simple techniques such as linear smoothing or median filtering which reduce noise but at the same time smooth away edges to a greater or lesser degree. By contrast, total variation denoising is remarkably effective at simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios.</p>
<h3 id="1D-signal-series"><a href="#1D-signal-series" class="headerlink" title="1D signal series"></a>1D signal series</h3><p>For a digital signal $y_n$, we can, for example, define the total variation as:</p>
<script type="math/tex; mode=display">V(y)=\sum_n\vert y_{n+1}-y_n\vert</script><p>Given an input signal $x_n$, the goal of total variation denoising is to find an approximation, call it $y_n$, that has smaller total variation than $x_n$ but is “close” to $x_n$. One measure of closeness is the sum of square errors:</p>
<script type="math/tex; mode=display">E(x, y)=\frac{1}{2}\sum_n(x_n - y_n)^2</script><p>So the total variation denoising problem amounts to minimizing the following discrete functional over the signal $y_n$:</p>
<script type="math/tex; mode=display">E(x, y) + \lambda V(y)</script><p>By differentiating this functional with respect to $y_n$, we can derive a corresponding Euler-lagrange equation, that can be numerically integrated with the original signal $x_n$ as initial condition. This was the original approach. Alternatively, since this is a convex functional, techniques from convex optimization can be used to minimize it and find the solution $y_n$.</p>
<h3 id="Regularization-properties"><a href="#Regularization-properties" class="headerlink" title="Regularization properties"></a>Regularization properties</h3><p>The regularization parameter $\lambda $ plays a critical role in the denoising process. When $\lambda = 0$, there is no smoothing and the result is the same as minimizing the sum of squares. As $\lambda \to \infty $, however, the total variation term plays an increasingly strong role, which forces the result to have smaller total variation, at the expanse of being less like the input (noisy) signal. Thus, the choice of regularization parameter is critical to achieving just the right amount of noise removal.</p>
<h3 id="2D-signal-images"><a href="#2D-signal-images" class="headerlink" title="2D signal images"></a>2D signal images</h3><p>We now consider 2D signals $y$, such as images. The total variation norm proposed by the 1992 paper is</p>
<script type="math/tex; mode=display">V(y) = \sum_{i,j}\sqrt{\vert y_{i+1,j}-y_{i,j}\vert ^2 + \vert y_{i, j+1} - y_{i, j}\vert ^2}</script><p>and is isotropic and not differentiable. A variation that is sometimes used, since it may sometimes be easier to minimize, is an anisotropic version</p>
<script type="math/tex; mode=display">V_{aniso}(y) = \sum_{i,j}\sqrt{\vert y_{i+1,j}-y_{i,j}\vert ^2} + \sqrt{\vert y_{i,j+1} - y_{i,j}\vert ^2} = \sum_{i,j}\vert y_{i+1,j}-y_{i,j}\vert + \vert y_{i,j+1}-y_{i,j}\vert</script><p>The standard total variation denoising problem is still of the form</p>
<script type="math/tex; mode=display">\min_yE(x,y)+\lambda V(y)</script><p>where $E$ is the 2D L2 norm. In contrast to the 1D case, solving this denoising is non-trivial. A recent algorithm that solves this is known as the primal dual method.<br>Due in part to much research in compressed sensing in the mid-2000s, there are many algorithms, such as the split-Bregman method, that solve variants of this problem.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/blog/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/blog/tags/computer-vision/" rel="tag"># computer vision</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/02/24/leetcode-algorithms-3/" rel="next" title="leetcode algorithms #3">
                <i class="fa fa-chevron-left"></i> leetcode algorithms #3
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Davidham</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Image-Style-Transfer-Using-Convolutional-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Image Style Transfer Using Convolutional Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#大体原理"><span class="nav-number">2.</span> <span class="nav-text">大体原理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-image-representations"><span class="nav-number">3.</span> <span class="nav-text">Deep image representations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#content-representation"><span class="nav-number">3.1.</span> <span class="nav-text">content representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#style-representation"><span class="nav-number">3.2.</span> <span class="nav-text">style representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#style-transfer"><span class="nav-number">3.3.</span> <span class="nav-text">style transfer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Results"><span class="nav-number">4.</span> <span class="nav-text">Results</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Trade-off-between-content-and-style-matching"><span class="nav-number">4.1.</span> <span class="nav-text">Trade-off between content and style matching</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#implementation"><span class="nav-number">5.</span> <span class="nav-text">implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Total-variation-denoising"><span class="nav-number">5.1.</span> <span class="nav-text">Total variation denoising</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1D-signal-series"><span class="nav-number">5.1.1.</span> <span class="nav-text">1D signal series</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization-properties"><span class="nav-number">5.1.2.</span> <span class="nav-text">Regularization properties</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2D-signal-images"><span class="nav-number">5.1.3.</span> <span class="nav-text">2D signal images</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Davidham</span>

  

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Gemini</a> v6.0.0</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=6.0.0"></script>



  
  


  <script type="text/javascript" src="/blog/js/src/affix.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=6.0.0"></script>



  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=6.0.0"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=6.0.0"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=6.0.0"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
