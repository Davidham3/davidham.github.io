<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=6.0.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=6.0.0">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=6.0.0" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">
<meta property="og:type" content="website">
<meta property="og:title" content="tags">
<meta property="og:url" content="http://yoursite.com/tags/index.html">
<meta property="og:site_name" content="Davidham&#39;s blog">
<meta property="og:description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-02-19T13:29:57.449Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tags">
<meta name="twitter:description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Gemini',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/tags/"/>





  <title>卷积的序列到序列学习 | Davidham's blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c30fad310ff0f715b4bafabc30583f7d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Davidham's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">修电脑的</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/blog/2018/05/22/卷积的序列到序列学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Davidham">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Davidham's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积的序列到序列学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-22T11:30:51+08:00">2018-05-22</time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/论文阅读笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>ICML 2017. Facebook 2017年的卷积版seq2seq。卷积加注意力机制，外加GLU，训练速度很快，因为RNN训练时依靠上一个元素的隐藏状态，CNN可以并行训练。<br><a id="more"></a></p>
<p>原文：<a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>流行的序列到序列的学习方法将输入的序列通过一个循环神经网络映射到一个可变长度的输出序列。我们引入了一个完全基于卷积的神经网络。对比循环神经网络模型，在训练过程中对所有元素的运算都可以完全并行化，而且可以充分利用GPU资源，优化过程也会变得更加容易，因为非线性单元的个数是保持不变的，而且与输入长度无关。我们使用的门控线性单元（GLU）可以帮助梯度传播，我们在每个解码层上部署了一个分离的注意力模块。我们算法表现在WMT’14英语-德语和WMT’14英语-法语两个数据集上，都要比Wu等人的深度LSTM拥有更高的精度，且在GPU和CPU上训练速度都快了一个量级。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>序列到序列模型在很多任务中都大获成功，如机器翻译、语音识别（Sutskever et al., 201v4; Chorowski et al., 2015），文本摘要（Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016）。当今主流的方法将输入序列使用一个双向循环神经网络进行编码，并且用一个循环神经网络生成一个可变长的输出序列，这两个接口都使用了软注意力机制（Bahdanau et al., 2014; Luong et al., 2015）。在机器翻译中，这个架构比传统的大边界阶段型模型（Sennrich et al., 2016b; Zhou et al., 2016; Wu et al., 2016）表现的更好。<br>卷积神经网络尽管有很多优点，但其在序列模型中并不是很常见（Waibel et al., 1989; LeCun &amp; Bengio, 1995）。对比循环神经网络，卷积可以对定长的内容生成表示，然而，有效的内容长度可以很简单的通过堆叠卷积层变得逐渐增加。这就使得我们可以通过建模的方法来精确地控制依赖之间的最大长度。卷积神经网络不需要依赖于之前时间步的计算，因此可以在序列中的任意一个地方进行并行运算。对比RNN，RNN需要维持一个含有整个过去信息的隐藏状态，导致对序列进行计算时不能并行。<br>多层卷积神经网络在整个输入序列上创建了层次表示，在这个层次表示中，先输入的元素在底层进行交互，而后输入的元素在高层进行交互。层级结构相比于链式结构的循环神经网络，提供了一条更短的路径来捕获长范围的依赖关系。比如，我们可以用一个n个单词的滑动窗，使用复杂度为$O(\frac{n}{k})$的卷积操作，卷积核宽度为k，来获取单词间的关系，提取到一个特征表示，如果用循环神经网络，复杂度为$O(n)$。卷积神经网络的输入会被放入一个有着固定数目的卷积核和非线性单元的网络中，然而循环神经网络对第一个单词进行了n次操作和非线性变换后，对最后一个单词只进行了一组操作。加在输入上的固定数目的非线性操作也可以简化学习过程。<br>最近在卷积神经网络应用于序列建模的工作，像Bradbury et al.(2016)，他们在一连串的卷积层间引入了循环池。Kalchbrenner et al.(2016)解决了没有注意力机制的神经机器翻译。然而，这些方法的表现没有一个在大型的基准数据集上超越当前最先进的技术。门控卷积在之前已经被Meng et al.(2015)用于了机器翻译，但是他们的评估方法受限于小数据集，而且模型与传统的基于计数的模型相串联。部分是卷积层的架构在大数据集上展现了更好的效果但是他们的解码器仍然是循环的。<br>在这篇文章中，我们提出了一个针对序列到序列的模型，这个模型完全是卷积的。我们的模型使用了门控线性单元（Dauphin et al., 2016）和残差连接（He et al., 2015a）。我们在每个解码层也使用了注意力机制，而且显示出每个注意力层只加了可以忽略不计的overhead。这些方法的融合可以让我们处理大规模的数据集。<br>我们在多个大型的机器翻译数据集上评估了我们的方法，并且和文章中现有的最好的架构们进行了对比与总结。在WMT’16英语-罗马尼亚语数据集上，我们的算法是最先进的，比之前最好的结果要好1.9 BLEU。在WMT’14英语-德语上，我们比Wu et al.(2016)的强LSTM好0.5 BLEU。在WMT’14英语-法语上，我们比Wu et al.(2016)的可能性训练系统好1.6 BLEU。除此以外，我们的模型可以翻译从未见过的句子，速度比Wu et al.(2016)的算法在GPU和CPU上都快出一个数量级。</p>
<h2 id="循环神经网络处理序列到序列"><a href="#循环神经网络处理序列到序列" class="headerlink" title="循环神经网络处理序列到序列"></a>循环神经网络处理序列到序列</h2><p>序列到序列建模又称基于循环神经网络的编码器-解码器架构（Sutskever et al., 2014; Bahdanau et al., 2014）。编码器RNN处理一个输入序列$x=(x_1, …, x_m)$m个元素，返回状态表示$z=(z_1, …, z_m)$。解码器RNN接受z并且从左到右生成输出序列$y=(y_1, …, y_n)$，一次一个元素。为了生成输出$y_{i+1}$，解码器基于之前的隐藏状态$h_i$计算了一个隐藏状态$h_{i+1}$，前一个目标单词$y_i$的嵌入表示$g_i$，也就是一个源自编码器输出$z$的条件输入$c_i$。基于这个大体的规则，各种各样的编码器-解码器架构被相继提出，他们之间主要是在条件输入和RNN的类型上不同。<br>没有注意力机制的模型只考虑最后的编码状态$z_m$，通过对所有的$i$设定$c_i = z_m$（Cho et al., 2014），或是简单地将解码器的第一个隐藏状态初始化为$z_m$（Sutskever et al., 2014），后这种$c_i$没有用到。带注意力机制的架构（Bahdanau et al., 2014; Luong et al., 2015）在每个时间步计算$(z_1, …, z_m)$的加权求和得到$c_i$。求和时每一项的权重是指注意力分数，可以使网络在输出序列时聚焦于输入序列的不同部分。注意力分数本质上是通过比较每个编码器状态$z_j$和解码器前一状态$h_i$和最后的预测结果$y_i$计算得到；结果会在输入元素上做一个归一化。<br>当前流行的编解码器常用的模型是长短时记忆网络（LSTM; Hochreiter &amp; Schmidhuber, 1997）和门控循环单元（GRU; Cho et al., 2014）。这两个都是使用门控机制对Elman的RNN（Elman, 1990）进行了扩展，门控机制使得模型可以记得前一时间步获得的信息，以此来对长期依赖进行建模。大多数最近的方法也依赖于双向编码器来同时对过去和未来的上下文建立特征表示（Bahdanau et al., 2014; Zhou et al., 2016; Wu et al., 2016）。通常带有很多层的模型会依赖于残差网络的残差连接（He et al., 2015a; Zhou et al., 2016; Wu et al., 2016）。</p>
<h2 id="一个卷积架构"><a href="#一个卷积架构" class="headerlink" title="一个卷积架构"></a>一个卷积架构</h2><p>接下来我们介绍一个序列到序列建模的完全卷积架构。我们使用卷积神经网络替代了使用RNN对计算编码器中间状态$z$和解码器状态$h$。</p>
<h3 id="位置嵌入"><a href="#位置嵌入" class="headerlink" title="位置嵌入"></a>位置嵌入</h3><p>首先，我们将输入序列$x = (x_1, …, x_m)$嵌入到$w = (w_1, …, w_m)$中，其中$w_j \in \mathbb{R}^f$是嵌入矩阵$\mathcal{D} \in \mathbb{R}^{V \times f}$的一列。我们也通过嵌入输入元素$p=(p_1, …, p_m)$的绝对位置使模型对顺序敏感，其中$p_j \in \mathbb{R}^f$。这两者做加和获得输入元素的表达$e=(w_1+p_1, …, w_m+p_m)$。我们对解码器输出的元素也做相似的工作来生成输出元素表示$g=(g_1, …, g_n)$，然后再传入解码网络。位置嵌入在我们的架构中很有用，因为他们给我们的模型一种可以感知当前正在处理的输入或输出序列中的哪个部分。</p>
<h3 id="卷积块结构"><a href="#卷积块结构" class="headerlink" title="卷积块结构"></a>卷积块结构</h3><p>编码器和解码器网络都分享一个简单的块状结构，这个块状结构计算的中间状态基于固定长度的输入元素。我们将解码器的第$l$个块的输出记为$h^l = (h^l_1, …, h^l_n)$，编码器的第$l$个输出记为$z^l=(z^l_1, …, z^l_m)$；我们交替的称块和层。每个块包含一个一维卷积加一个非线性层。对于一个有着一个块和宽度为$k$的卷积核的解码网络来说，每个结果状态$h^l_i$包含了过去$k$个输入元素的信息。堆叠多个块增加了在一个状态中表达的输入元素的数量。比如，堆叠6个宽度为$k=5$的块可以得到输入域为25个元素，也就是每个输出依赖于25个输入。非线性单元允许网络利用整个输入域，或者聚焦几个元素。<br>每个卷积核的参数为$W \in \mathbb{R}^{2d \times kd}$，$b_w \in \mathbb{R}^{2d}$，接收的输入为$X \in \mathbb{R}^{k \times d}$，输入是$k$个输入元素嵌入到$d$维空间并且将他们映射到一个有着输入元素维数两倍的输出元素$Y \in \mathbb{R}^{2d}$；后续的层对前一层的$k$个元素进行操作。我们选择门控线性单元（GLU; Dauphin et al., 2016）作为非线性激活单元，这是一个在卷积$Y=[A B] \in \mathbb{R}^{2d}$的输出上实现的一个简单的门控机制：</p>
<script type="math/tex; mode=display">v([A B])=A \otimes \sigma(B)</script><p>其中$A, B \in \mathbb{R}^d$是非线性层的输入，$\otimes$是元素对元素相乘，输出$v([A B]) \in \mathbb{R}^d$是$Y$的大小的一半。门$\sigma(B)$控制当前的上下文中哪个输入$A$是有关的。一个相似的非线性单元由Oord et al. (2016b)引入，他们在$A$上加了$tanh$但是Dauphin et al. (2016)展示了GLUs在语言模型中表现的更好。<br>为了让深度卷及网络可行，我们在每个卷积的输入和块的输出间加入了残差连接（He et al., 2015a）。</p>
<script type="math/tex; mode=display">h^l_i=v(W^l[h^{l-1}_{i-k/2}, ..., h^{l-1}_{i+k/2}] + b^l_w) + h^{l-1}_i</script><p>对于编码器网络，我们确信卷积层的输出通过在每层增加padding可以匹配输入长度。然而，对于解码器网络我们需要注意对于解码器来说没有可用的未来信息（Oord et al., 2016a）。特别地，我们在输入的左右两侧都加了$k-1$个0作为padding，并且从卷积输出的末端删除了$k$个元素。<br>We also add linear mappings to project between the embedding size $f$ and the convolution outputs that are of size $2d$.我们在将嵌入表示输入到编码器的时候，对$w$进行了这样的变换，也对编码器输出$z^y_j$，解码器在softmax$h^L$之前的最后一层，以及计算注意力值之前的解码器的每一层$h^l$。<br>最后，我们计算了在$T$个可能的下一个目标元素$y_{i+1}$上的分布，通过将解码器最上面的输出做线性变换得到：</p>
<script type="math/tex; mode=display">p(y_{i+1} \mid y_1, ..., y_i, x) = softmax(W_oh^L_i+b_o) \in \mathbb{R}^T</script><h3 id="多步注意力机制"><a href="#多步注意力机制" class="headerlink" title="多步注意力机制"></a>多步注意力机制</h3><p>我们对于每一个解码层引入了一个分开的注意力机制。为了计算注意力，我们融合了当前解码器状态$h^l_i$和前一个目标元素$g_i$的嵌入表示：</p>
<script type="math/tex; mode=display">d^l_i=W^l_dh^l_i + b^l_d + g_i</script><p>解码器层$l$的注意力$a^l_{ij}$的状态$i$和源元素$j$是通过解码器状态的汇总$d^l_i$和最后一个编码器块$u$的每个输出$z^u_j$的点乘得到：</p>
<script type="math/tex; mode=display">a^l_{ij}=\frac{\exp(d^l_i \cdot z^u_j)}{\sum^m_{t=1}\exp(d^l_i \cdot z^u_t)}</script><p>对当前解码器层的条件输入$c^l_i$是编码器输出的加权求和，也就是输入元素嵌入$e_j$（图1，中右侧）：</p>
<script type="math/tex; mode=display">c^l_i=\sum^m_{j=1}a^l_{ij}(z^u_j+e_j)</script><p>这与循环神经网络的方法稍有不同，在循环神经网络中，注意力和$z^u_j$的加权求和是同时计算的(This is slightly different to recurrent approaches which compute both the attention and the weighted sum over $z^u_j$ only)。我们发现加入$e_j$更有效，而且它与键值记忆网络相似，后者的键是$z^u_j$，值是$z^u_j+e_j$（Miller et al., 2016）。编码器输出$z^u_j$潜在地表达了大量的输入上下文，$e_j$提供了一个在做预测时有效的输入元素的点信息。一旦$c^l_i$被计算得出，它就会被简单的加到对应解码层$h^l_i$的输出上。<br>对比单步注意力机制（Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016）来说，这可以被看作是多步“跳跃”（Sukhbaater et al., 2015）的注意力机制。特别地，第一层的注意力决定了被输入到第二层的一个有效的源上下文，第二层在计算注意力的时候考虑了这个信息。解码器也可以直接获取注意力的前$k-1$步历史，因为条件输入$c^{l-1}_{i-k}, …, c^{l-1}_{i}$是$h^{l-1}_{i-k}, …, h^{l-1}_i$的一部分，而后者是$h^l_i$的输入。对于循环神经网络来说，前几步的输入在隐藏状态中，并且需要经过多个非线性单元后还保存下来，但是对于卷积的网络来说，考虑已经被加入的前几步信息更加简单。总的来说，我们的注意力机制考虑了我们之前已经加入的哪个单词，并且每个时间步都实现了多次跳跃注意力机制。在后记$C$中，我们绘制了对于深层解码器的注意力分数，显示出了不同层，被考虑的源输入的不同位置。<br>我们的卷积架构与RNN相比可以批量的计算一个序列的所有元素的注意力（图1，中间）。我们分别地计算了每个解码器层。</p>
<h3 id="正则化的策略"><a href="#正则化的策略" class="headerlink" title="正则化的策略"></a>正则化的策略</h3><p>我们通过小心的权重初始化稳定了学习过程$\text{\S3.5}$，并且通过缩放网络的部分使透过网络中的方差不会剧烈的变化。特别地，我们也对残差块的输出和注意力进行缩放来保持激活后的方差。我们将输入的加和和一个残差块的输出乘以了$\sqrt{0.5}$，来使和的方差减半。这假设了被加数有着相同的方差，虽说不一定总是能这样，但是在使用的时候还是很有效的。<br>通过注意力机制生成的条件输入$c^l_i$是$m$个向量的加权求和，我们通过缩放$m\sqrt{1/m}$抵消了在方差上的一个变化；我们假设注意力分数是均匀分布的，对输入乘以了$m$来使他从原来的大小放大。一般我们不这么干，但是我们发现在实际情况中表现得还挺好。<br>对于带有多个注意力机制的卷积解码器，我们根据我们的使用的注意力机制的数量对编码层的梯度进行缩放；我们排除了源单词的嵌入。我们发现这样可以稳定训练过程，因为如果不这样编码器就会接受到很大的梯度。</p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>在对不同层的输出进行加和的时候，比如残差连接，对激活单元进行归一化时需要很小心的权重初始化。我们的初始化策略是受到了归一化的启发：保持网络前向和反向传播时激活后的值的方差。所有的嵌入表示从一个均值为0，标准差为0.1的正态分布中初始化得到。对于输出不直接输入到门控线性单元的层，我们从$\mathcal{N}(0, \sqrt{1/n_l})$中初始化权重，其中$n_l$是每个神经元输入连接数。这确保了一个均匀分布输入的方差是保持不变的。<br>对于通过GLU激活的层来说，我们提出了一个通过adapting the derivations in（He et al., 2015b; Glorot &amp; Bengio, 2010; Appendix A）的初始化机制。如果GLU的输入服从均值为0的分布，并且有充分小的方差，那么我们可以用输入方差的四分之一来近似输出方差（Appendix A.1）。因此，我们初始化权重，使得GLU激活的输入有着4倍的输入的方差。这可以通过对$\mathcal{N}(0, \sqrt{r/n_l})$采样初始化得到。偏置项在网络构建时均匀的设置为0。<br>我们在某些层的输入使用了dropout，以便输入保持一个概率$p$。这可以看作是一个伯努利随机变量取值为$1/p$的概率为$p$，其他为0（Srivastava et al., 2014）。dropout的应用会使得方差被$1/p$缩放。我们的目标是通过使用大的权重来初始化各个层来恢复进入的方差(We aim to restore the incoming variance by initializing the respective layers with larger weights)。特别地，我们使用$\mathcal{N}(0, \sqrt{4p/n_l})$初始化那些输出会输入至GLU的层，使用$\mathcal{N}(0, \sqrt{p/n_l})$来初始化其他的。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/blog/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/blog/tags/machine-translation/" rel="tag"># machine translation</a>
          
            <a href="/blog/tags/seq2seq/" rel="tag"># seq2seq</a>
          
            <a href="/blog/tags/icml/" rel="tag"># ICML</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/05/10/时空图卷积在交通流预测上的应用/" rel="next" title="时空图卷积在交通流预测上的应用">
                <i class="fa fa-chevron-left"></i> 时空图卷积在交通流预测上的应用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2018/05/23/门控卷积网络语言建模/" rel="prev" title="门控卷积网络语言建模">
                门控卷积网络语言建模 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNzEwMS8xMzYzNw=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/blog/uploads/avatar.jpg"
                alt="Davidham" />
            
              <p class="site-author-name" itemprop="name">Davidham</p>
              <p class="site-description motion-element" itemprop="description">Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Davidham3" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://weibo.com/Davidham3" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-globe"></i>微博</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#引言"><span class="nav-number">2.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络处理序列到序列"><span class="nav-number">3.</span> <span class="nav-text">循环神经网络处理序列到序列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一个卷积架构"><span class="nav-number">4.</span> <span class="nav-text">一个卷积架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#位置嵌入"><span class="nav-number">4.1.</span> <span class="nav-text">位置嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积块结构"><span class="nav-number">4.2.</span> <span class="nav-text">卷积块结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多步注意力机制"><span class="nav-number">4.3.</span> <span class="nav-text">多步注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化的策略"><span class="nav-number">4.4.</span> <span class="nav-text">正则化的策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化"><span class="nav-number">4.5.</span> <span class="nav-text">初始化</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Davidham</span>

  

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Gemini</a> v6.0.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>





  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65896580";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=6.0.0"></script>



  
  


  <script type="text/javascript" src="/blog/js/src/affix.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=6.0.0"></script>



  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=6.0.0"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=6.0.0"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=6.0.0"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
